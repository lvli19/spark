现在我们介绍更多关于RDD编程的内容。

__Spark中针对RDD的操作包括创建RDD、RDD转换操作和RDD行动操作。__

# RDD创建
RDD可以通过两种方式创建：
* 第一种：读取一个 __外部数据集__ 。比如，从本地文件加载数据集，或者从HDFS文件系统、HBase、Cassandra、Amazon S3等外部数据源中加载数据集。
Spark可以支持文本文件、SequenceFile文件（Hadoop提供的 SequenceFile是一个由二进制序列化过的key/value的字节流组成的文本存储文件）
和其他符合Hadoop InputFormat格式的文件。
* 第二种：调用SparkContext的parallelize方法，在Driver中一个已经存在的集合（数组）上创建。
创建RDD之前的准备工作
在即将进行相关的实践操作之前，我们首先要登录Linux系统（本教程统一采用hadoop用户登录），然后，打开命令行“终端”，请按照下面的命令启动Hadoop中的HDFS组件：
