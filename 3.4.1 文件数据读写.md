除了可以对本地文件系统进行读写以外，Spark还支持很多其他常见的文件格式（如文本文件、JSON、SequenceFile等）和文件系统（如HDFS、Amazon S3等）和数据库（如MySQL、HBase、Hive等）。数据库的读写我们将在Spark SQL部分介绍，因此，这里只介绍文件系统的读写和不同文件格式的读写。
# 文件系统的数据读写
下面分别介绍 __本地文件系统的数据读写和分布式文件系统HDFS的数据读写__ 。
## 本地文件系统的数据读写
首先，请在第二个终端窗口下操作，用下面命令到达“/usr/local/spark/mycode/wordcount”目录，查看一下上面已经建好的word.txt的内容：
```
cd /usr/local/spark/mycode/wordcount
cat word.txt
```

```
>>> textFile = sc.textFile("file:///usr/local/spark/mycode/wordcount/word.txt")
```
注意，要加载本地文件，必须采用“file:///” 开头的这种格式。执行上上面这条命令以后，并不会马上显示结果，因为，Spark采用惰性机制，只有遇到“行动”类型的操作，才会从头到尾执行所有操作。所以，下面我们执行一条“行动”类型的语句，就可以看到结果：
```
>>> textFile.first()
```
好了，现在我们可以练习一下如何 __把textFile变量中的内容再次写回到另外一个文本文件wordback.txt中：__

```
>>> textFile = sc.textFile("file:///usr/local/spark/mycode/wordcount/word.txt")
>>> textFile.saveAsTextFile("file:///usr/local/spark/mycode/wordcount/writeback.txt")
```

## 分布式文件系统HDFS的数据读写
为了能够读取HDFS中的文件，请首先启动Hadoop中的HDFS组件。
```
cd /usr/local/hadoop
./sbin/start-dfs.sh
```
```
./bin/hdfs dfs -mkdir -p /user/hadoop
```

也就是说，HDFS文件系统为Linux登录用户开辟的默认目录是“/user/用户名”（注意：是user，不是usr），本教程统一使用用户名hadoop登录Linux系统，所以，上面创建了“/user/hadoop”目录，再次强调，这个目录是在HDFS文件系统中，不在本地文件系统中。创建好以后，下面我们使用命令查看一下HDFS文件系统中的目录和文件：
```
./bin/hdfs dfs -ls .
```
*注释： mkdir -p ：递归创建目录，即使上级目录不存在，会按目录层级自动创建目录*

上面命令中， __最后一个点号“.”，表示要查看Linux当前登录用户hadoop在HDFS文件系统中与hadoop对应的目录下的文件__ ，也就是查看HDFS文件系统中“/user/hadoop/”目录下的文件，所以，下面两条命令是等价的：
```
./bin/hdfs dfs -ls .
./bin/hdfs dfs -ls /user/hadoop
```

如果要查看HDFS文件系统根目录下的内容，需要使用下面命令

```
./bin/hdfs dfs -ls /
```

下面，我们把本地文件系统中的“/usr/local/spark/mycode/wordcount/word.txt”上传到分布式文件系统HDFS中（放到hadoop用户目录下）：
```
./bin/hdfs dfs -put /usr/local/spark/mycode/wordcount/word.txt .
```

然后，用命令查看一下HDFS的hadoop用户目录下是否多了word.txt文件，可以使用下面命令列出hadoop目录下的内容：
```
./bin/hdfs dfs -ls .
```
```
./bin/hdfs dfs -cat ./word.txt
```
现在，让我们切换回到pyspark窗口，编写语句从HDFS中加载word.txt文件，并显示第一行文本内容：

```
>>> textFile = sc.textFile("hdfs://localhost:9000/user/hadoop/word.txt")
>>> textFile.first()
```

执行上面语句后，就可以看到HDFS文件系统中（不是本地文件系统）的word.txt的第一行内容了。
需要注意的是，sc.textFile(“hdfs://localhost:9000/user/hadoop/word.txt”)中，
“hdfs://localhost:9000/”是前面介绍Hadoop安装内容时确定下来的端口地址9000。
实际上，也可以省略不写，如下三条语句都是等价的：
```
>>> textFile = sc.textFile("hdfs://localhost:9000/user/hadoop/word.txt")
>>> textFile = sc.textFile("/user/hadoop/word.txt")
>>> textFile = sc.textFile("word.txt")
```
下面，我们再 __把textFile的内容写回到HDFS文件系统中（写到hadoop用户目录下）：__
```
>>> textFile = sc.textFile("word.txt")
>>> textFile.saveAsTextFile("writeback.txt")
```


# 不同文件格式的读写
## 文本文件
如上 .textFile(""), .saveAsTextFile("")

## JSON
JSON(JavaScript Object Notation) 是一种轻量级的数据交换格式。
Spark提供了一个JSON样例数据文件，存放在“/usr/local/spark/examples/src/main/resources/people.json”中。
people.json文件的内容如下：
```
{"name":"Michael"}
{"name":"Andy", "age":30}
{"name":"Justin", "age":19}
```
我们下面可以对这个样例数据文件进行解析。
下面请在Linux系统的Shell命令提示符下操作，请进入“/usr/local/spark/mycode”目录，并新建一个json子目录，代码如下：

```
cd /usr/local/spark/mycode
mkdir json
cd json
```
在编写解析程序之前，我们首先来看一下把本地文件系统中的people.json文件加载到RDD中以后，数据是什么形式，请在spark-shell中执行如下操作：
```
>>> jsonStr = sc.textFile("file:///usr/local/spark/examples/src/main/resources/people.json")
>>> jsonStr.foreach(print)
{"name":"Michael"}
{"name":"Andy", "age":30}
{"name":"Justin", "age":19}
```
从上面执行结果可以看出，people.json文件加载到RDD中以后，在RDD中存在三个字符串。
我们下面要做的事情，就是把这三个JSON格式的字符串解析出来，
比如说，第一个字符串{“name”:”Michael”}，经过解析后，解析得到key是”name”，value是”Michael”。
现在我们编写程序完成对上面字符串的解析工作。
Scala中有一个自带的JSON库——scala.util.parsing.json.JSON，可以实现对JSON数据的解析。
JSON.parseFull(jsonString:String)函数，以一个JSON字符串作为输入并进行解析，
如果解析成功则返回一个Some(map: Map[String, Any])，如果解析失败则返回None。
因此，我们可以使用模式匹配来处理解析结果
```
cd /usr/local/spark/mycode/json
vim testjson.py
```

在testjson.py代码文件中输入以下内容：
```from pyspark import SparkContext
import json
sc = SparkContext('local','JSONAPP')
inputFile =  "file:///home/dblab/people.json"
jsonStrs = sc.textFile(inputFile)
result = jsonStrs.map(lambda s : json.loads(s))
result.foreach(print)    
```

接下请执行如下代码：
```
python3 testjson.py
```

