 __DStream是Spark Streaming的编程模型，DStream的操作包括输入、转换和输出。__

# Spark Streaming工作原理
前面在《Spark运行架构》部分，我们已经介绍过，在Spark中，一个应用（Application）由一个任务控制节点（Driver）和若干个作业（Job）构成，
一个作业由多个阶段（Stage）构成，一个阶段由多个任务（Task）组成。
当执行一个应用时，任务控制节点会向集群管理器（Cluster Manager）申请资源，
启动Executor，并向Executor发送应用程序代码和文件，然后在Executor上执行task。

在Spark Streaming中，会有一个组件Receiver，作为一个长期运行的task跑在一个Executor上。
每个Receiver都会负责一个input DStream（比如从文件中读取数据的文件流，比如套接字流，或者从Kafka中读取的一个输入流等等）。
* Spark Streaming通过 __input DStream与外部数据源进行连接，读取相关数据__ 。

# Spark Streaming程序基本步骤
## 编写Spark Streaming程序的基本步骤是：

__1.通过创建输入DStream来定义输入源__

__2.通过对DStream应用转换操作和输出操作来定义流计算。__

__3.用streamingContext.start()来开始接收数据和处理流程。__

__4.通过streamingContext.awaitTermination()方法来等待处理结束（手动结束或因为错误而结束）。__

__5.可以通过streamingContext.stop()来手动结束流计算进程。__

# 创建StreamingContext对象

如果要运行一个Spark Streaming程序，就需要首先生成一个StreamingContext对象，它是Spark Streaming程序的主入口。
因此，在定义输入之前，我们首先介绍如何创建 __StreamingContext__ 对象。
我们可以从一个SparkConf对象创建一个StreamingContext对象。


请登录Linux系我们首先介绍如何创建StreamingContext对象统，启动pyspark。
进入pyspark以后，就已经获得了一个默认的SparkConext，也就是sc。
因此，可以采用如下方式来创建StreamingContext对象：
```
>>> from pyspark import SparkContext
>>> from pyspark.streaming import StreamingContext
>>> ssc = StreamingContext(sc, 1)
```
_1表示每隔1秒钟就自动执行一次流计算，这个秒数可以自由设定。_

如果是编写一个独立的Spark Streaming程序，而不是在pyspark中运行，则需要通过如下方式创建StreamingContext对象：
```
from pyspark import SparkContext, SparkConf
from pyspark.streaming import StreamingContext
conf = SparkConf()
conf.setAppName('TestDStream')
conf.setMaster('local[2]')
sc = SparkContext(conf = conf)
ssc = StreamingContext(sc, 1)
```
setAppName(“TestDStream”)是用来设置应用程序名称，这里我们取名为“TestDStream”。

setMaster(“local[2]”)括号里的参数”local[2]’字符串表示运行在本地模式下，并且启动2个工作线程。
